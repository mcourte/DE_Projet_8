# üå¶Ô∏è Pipeline ETL : R√©f√©rentiel Stations M√©t√©o (Projet 8)

Ce projet automatise l'extraction, la transformation et le stockage du r√©f√©rentiel des stations m√©t√©o d'InfoClimat. Le pipeline assure la transition s√©curis√©e des donn√©es depuis une source brute vers une base de donn√©es NoSQL MongoDB configur√©e en haute disponibilit√©.

## üèóÔ∏è Architecture & Logique de Migration

Le processus suit une architecture **ETL** (Extract, Transform, Load) structur√©e pour garantir l'int√©grit√© des donn√©es :



### 1. Extraction (Source vers S3)
* **Source** : Donn√©es brutes issues d'InfoClimat (via Google Drive).
* **Transport** : Synchronisation via **Airbyte** vers un bucket **AWS S3** (format CSV).
* **Formatage** : Les donn√©es sont extraites dynamiquement depuis la colonne brute `_airbyte_data`.

### 2. Transformation (Python & Pandas)
Le script `transform_and_load.py` effectue les op√©rations critiques suivantes :
* **Nettoyage (Data Quality)** : Sur les 1157 lignes initiales, le script identifie et supprime les doublons (1152 lignes redondantes √©limin√©es).
* **Filtrage m√©tier** : Seules les **4 stations de r√©f√©rence** sont conserv√©es pour isoler le r√©f√©rentiel du bruit de mesures.
* **Validation** : V√©rification stricte du typage (coordonn√©es en floats) et de la pr√©sence des colonnes critiques (`id`, `name`, `latitude`, `longitude`).

### 3. Stockage & R√©plication (MongoDB)
* **CRUD** : Impl√©mentation des op√©rations de cr√©ation, lecture et mise √† jour.
* **Haute Disponibilit√©** : Les donn√©es sont stock√©es sur un **Replica Set** (`rs0`), garantissant la tol√©rance aux pannes et la persistance des donn√©es sur plusieurs n≈ìuds.

---

## üîç Observabilit√© & Qualit√© des Donn√©es

Afin de valider la robustesse du pipeline, trois scripts d'audit ont √©t√© d√©ploy√©s :

### üõ°Ô∏è Audit d'Int√©grit√© (`audit_integrity.py`)
| Indicateur | Source (S3) | Cible (MongoDB) | Statut |
| :--- | :--- | :--- | :--- |
| **Volume de lignes** | 1157 | **4** | ‚úÖ Filtrage OK |
| **Doublons (IDs)** | 1152 | **0** | ‚úÖ D√©doublonnage OK |
| **Valeurs manquantes**| 4580 | **0** | ‚úÖ Nettoyage OK |
| **Taux d'erreur final**| - | **0.00%** | ‚úÖ Int√©grit√© Totale |



### ‚è±Ô∏è Mesure de Performance (`temp_access.py`)
* **Temps d'accessibilit√© moyen** : **45.56 ms**.
* **Analyse** : Ce temps de r√©ponse quasi-instantan√© valide l'indexation et l'efficacit√© de la structure NoSQL pour des requ√™tes fr√©quentes.

### üîÑ Test de R√©plication (`test_replication.py`)
* **√âtat du Cluster** : D√©tection automatique du n≈ìud `PRIMARY`.
* **V√©rification** : Test de "Write Propagation" r√©ussi (la donn√©e √©crite sur le ma√Ætre est imm√©diatement disponible en lecture).

---

## üöÄ Installation et Utilisation

### Pr√©-requis
* Python 3.12+
* MongoDB configur√© en mode Replica Set (`--replSet rs0`)
* Fichier `.env` configur√© avec vos acc√®s AWS S3.

### Ex√©cution
1. Pr√©paration de l'environnement

```
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
2. Lancement du Pipeline ETL
```
python3 transform_and_load.py
```
3. Ex√©cution des Audits de Qualit√©
```
python3 audit_integrity.py
python3 temp_access.py
```
  
## üõ†Ô∏è Logigramme du Processus
**D√©but**: Lancement du script Python.  
**Entr√©e** : R√©cup√©ration du CSV sur AWS S3 via ```boto3```.  
**Traitement** : Extraction JSON et nettoyage via ```pandas```.  
**D√©cision** : Test d'int√©grit√© (Doublons/Nulls).  
**Stockage** : ```insert_many``` vers MongoDB (Replica Set).  
**Fin** : G√©n√©ration du JSON de rendu.  
  
## üìù Rapport Final : rendu_final_stations.json

Exemple du contenu attendu apr√®s ex√©cution de ton script d'export.

```
[
    {
        "Weather Station ID": "07015",
        "Station Name": "Lille-Lesquin",
        "Latitude / Longitude": "50.57¬∞ N, 3.09¬∞ E",
        "Elevation": "47",
        "City": "Lille",
        "State": "-/-",
        "Hardware": "other",
        "Software": "EasyWeatherPro_V5.1.6"
    }
]
```